---
kbId: 5135
title: Восстановление кластера {{ productName }}
tags:
    - Ignite
    - Grafana
    - Kafka
    - Loki
    - OpenSearch
    - Prometheus
    - администрирование
    - архитектура
    - балансировка нагрузки
    - восстановление
    - кластер
    - кластеризация
    - мониторинг
    - отказоустойчивость
    - развёртывание
    - резервное копирование
    - управление кластером
hide: tags
---

# Восстановление кластера {{ productName }} {: #cluster_recovery }

{% include-markdown ".snippets/experimental_feature.md" %}

## Введение {: #cluster_recovery_about }

**{{ productName }}** поддерживает кластерные конфигурации, которые обеспечивают высокую доступность, отказоустойчивость и горизонтальное масштабирование.

Здесь представлены рекомендации и процедуры восстановления ПО **{{ productName }}** в кластерных конфигурациях после сбоев.

### Основные принципы восстановления кластера {: #cluster_recovery_philosophy }

Кластерная архитектура **{{ productName }}** позволяет восстанавливать систему благодаря следующим возможностям:

- **Проверка здоровья узлов**: все узлы предоставляют эндпоинт `api/health` для мониторинга состояния.
- **Автоматическое перераспределение трафика**: балансировщик нагрузки должен автоматически исключать неисправные узлы.
- **Резервные узлы**: {{ apacheIgniteVariants }}, {{ openSearchVariants }} и {{ apacheKafkaVariants }} обеспечивают избыточность во время восстановления.
- **Распределённое хранилище**: DFS обеспечивает синхронизацию данных для всех узлов.

При восстановлении кластера **{{ productName }}** соблюдайте рекомендованный порядок:

1. **Диагностика проблемы**: определите причину сбоя и масштаб повреждений.
2. **Выберите стратегию восстановления**: восстановление одного узла или полное восстановление кластера.
3. **Выполните восстановление**: следуйте процедурам для выбранной стратегии.
4. **Проверьте работоспособность**: убедитесь, что все компоненты функционируют корректно.

### Архитектурные основы восстановления {: #cluster_recovery_architecture }

Кластерная архитектура **{{ productName }}** обеспечивает восстановление благодаря следующим компонентам:

**Распределённое хранилище данных {{ apacheIgniteVariants }}** — обеспечивает репликацию данных между узлами, высокую доступность и производительность обработки данных.

**Шина сообщений {{ apacheKafkaVariants }}** — обеспечивает межсервисное взаимодействие и обработку событий.

**Хранилище журналов событий {{ openSearchVariants }}** — обеспечивает сбор, индексирование и обработку журналов распределённых событий.

**Распределённая файловая система (DFS)** — общее хранилище файлов может быть реализовано на NFS или S3. Обеспечивает хранение загружаемых пользовательских файлов и других бинарных данных.

**Управление рабочими процессами** — только один узел выполняет критические фоновые задачи (резервное копирование, обработка процессов).

## Рекомендации по восстановлению кластера {: #cluster_recovery_strategies .pageBreakBefore }

- **Запланируйте восстановление**:
  - Определите причину сбоя и масштаб повреждений.
  - Выберите подходящую стратегию восстановления.
  - Подготовьте план действий и необходимые ресурсы.
- **Протестируйте восстановление в изолированной среде**:
  - Создайте тестовую копию кластера.
  - Протестируйте процедуры восстановления.
  - Проверьте совместимость восстановленного кластера с существующими данными.
- **Контролируйте работоспособность кластера в процессе восстановления**:
  - Отслеживайте состояние всех компонентов и сервисов.
  - Замеряйте производительность системы.
  - Будьте готовы к немедленному откату при необходимости.

## Ключевые этапы восстановления {: #cluster_recovery_key_principles .pageBreakBefore }

### Подготовка к восстановлению {: #cluster_recovery_preparation }

- Создайте резервную копию данных **{{ productName }}**, чтобы упростить восстановление кластера в случае проблем.
- Проверьте доступность и работоспособность {{ apacheIgniteVariants }}, {{ openSearchVariants }} и {{ apacheKafkaVariants }}, чтобы кластер сохранял работоспособность во время восстановления.
- Проанализируйте журналы на предмет наличия ошибок и предупреждений, которые могут помешать восстановлению.

### Мониторинг во время восстановления {: #cluster_recovery_monitoring }

- Постоянно контролируйте эндпоинт `api/health` на всех узлах, он должен возвращать статус `200`.
- Анализируйте журналы состояния **{{ productName }}** (`heartbeat_*.log`).
- Контролируйте журнал {{ apacheIgniteVariants }} (`igniteClient_*.log`).
- Контролируйте синхронизацию данных между узлами.

### Проверка после восстановления {: #cluster_recovery_verification }

- Дождитесь сообщения об окончании ребалансировки кластера.
- Найдите в журналах балансировщика сообщение `INFO Skipping rebalancing (nothing scheduled)`.
- Проверьте результирующую топологию {{ apacheIgniteVariants }}.

## Восстановление одного узла {: #cluster_recovery_single_node .pageBreakBefore }

### Полное восстановление кластера {: #cluster_recovery_complete_cluster }

### Подготовка к восстановлению

- Убедитесь в наличии актуальной резервной копии данных.
- Проверьте целостность резервных копий.
- Убедитесь в работоспособности {{ apacheIgniteVariants }}, {{ openSearchVariants }} и {{ apacheKafkaVariants }}.
- Проверьте доступность распределённой файловой системы (DFS).

### Восстановление данных

- Остановите все узлы кластера в правильном порядке.
- Восстановите данные из резервной копии на основной узел.
- Восстановите файлы из общего хранилища (NFS).
- Восстановите индексы {{ openSearchVariants }}.
- Восстановите конфигурационные файлы узлов.

### Запуск узлов

- Запустите узлы в порядке приоритета (сначала основной узел, затем дополнительные).
- Запустите службы в правильном порядке (приложение → API Gateway → Ignite → Nginx).
- Проверьте подключение каждого узла к кластерам.
- Дождитесь сообщения об окончании ребалансировки: `INFO Skipping rebalancing (nothing scheduled)`.

### Проверка восстановления

- Убедитесь, что все узлы возвращают статус `200` на эндпоинте `api/health`.
- Проверьте время отклика всех узлов
- Протестируйте балансировку нагрузки

**Протестируйте функциональность:**

- Протестируйте все основные функции системы
- Проверьте работу интеграций
- Мониторьте производительность всех компонентов

**Проверьте синхронизацию данных:**

- Убедитесь в синхронизации данных между узлами
- Проверьте репликацию в {{ apacheIgniteVariants }}
- Протестируйте операции чтения/записи

**Проверьте конфигурацию рабочих процессов:**

- Убедитесь, что только один узел имеет включённые службы `BackupSessionsQueue` и `ProcessEngineQueueProcessing`
- Проверьте файл `Workers.config` на всех узлах
- Убедитесь в правильности `ConfigurationId` на всех узлах

### Восстановление одного узла {: #cluster_recovery_single_node }

Когда отказывает один узел, остальные узлы кластера продолжают обрабатывать нагрузку. Данные остаются доступными через другие узлы, а конфигурация кластера не теряется.

**Типы узлов:**

- **Серверные узлы** — выполняют полную функциональность приложения и участвуют в кластере {{ apacheIgniteVariants }}.
- **Клиентские узлы** — работают в режиме тонкого клиента и подключаются к серверным узлам.
- **Узлы с рабочими процессами** — только один узел должен выполнять критические фоновые задачи.

**Стратегии восстановления:**

- **Быстрое восстановление** — перезапуск служб без изменения конфигурации.
- **Восстановление с синхронизацией** — восстановление данных из других узлов кластера.
- **Полное восстановление узла** — восстановление из резервной копии с последующей синхронизацией.

### Диагностика отказа узла

- Используйте эндпоинт `api/health` для проверки состояния узла.
- Анализируйте логи состояния экземпляра (`heartbeat_*.log`).
- Мониторьте логи {{ apacheIgniteVariants }} (`igniteClient_*.log`).
- Проверьте состояние файловой системы.
- Проверьте доступность сетевых ресурсов.

### Восстановление узла

- Остановите проблемный узел:
  ```sh
  systemctl stop comindware<instanceName>
  ```
- Исправьте выявленные проблемы.
- При необходимости восстановите из резервной копии.
- Проверьте конфигурацию узла.

- Запустите узел:
  ```sh
  systemctl start comindware<instanceName>
  ```

### Синхронизация с кластером

- Проверьте подключение к {{ apacheIgniteVariants }}:
  ```sh
  bash /usr/share/ignite/bin/control.sh --baseline
  ```
- Дождитесь сообщения: `INFO Skipping rebalancing (nothing scheduled)`.
- Проверьте топологию кластера.
- При восстановлении полного кластера создайте файл `hold.lock` в директории базы данных.
- Дождитесь завершения ребалансировки кластера.
- Удалите файл `hold.lock` для активации кластера.

### Проверка восстановления

- Убедитесь, что эндпоинт `api/health` возвращает статус `200`.
- Проверьте время отклика (не должно превышать 5 секунд).
- Анализируйте логи на предмет ошибок.
- Проверьте синхронизацию данных с другими узлами.
- Убедитесь, что критически важные службы активны только на одном узле.

## Мониторинг и диагностика {: #cluster_recovery_monitoring .pageBreakBefore }

### Мониторинг при восстановлении {: #cluster_recovery_monitoring_philosophy }

Во время восстановления система находится в переходном состоянии. Мониторинг помогает:
- Оценить прогресс восстановления
- Выявить новые проблемы, возникшие в процессе
- Принять решение о готовности системы к полноценной работе

### Ключевые индикаторы состояния {: #cluster_recovery_health_indicators }

**Эндпоинт `api/health`:**

- Статус `200 OK` — узел работает корректно
  - Время отклика не должно превышать 5 секунд
- Проверяйте регулярно, например каждые 30 секунд

**{{ apacheIgniteVariants }}:**

- Проверяйте топологию кластера
- Отслеживайте состояние репликации данных
- Контролируйте производительность операций чтения и записи

**{{ openSearchVariants }}:**

- Используйте эндпоинт `/_cluster/health` для проверки состояния
- Проверяйте состояние индексов и их репликации
- Отслеживайте производительность поисковых запросов

**{{ apacheKafkaVariants }}:**

- Контролируйте состояние топиков и партиций
- Отслеживайте задержки обработки сообщений
- Проверяйте размер очередей сообщений

### Диагностика проблем {: #cluster_recovery_diagnosis }

**Проверьте состояние узлов:**

- Используйте эндпоинт `api/health` для проверки состояния узлов приложений
- Анализируйте логи состояния экземпляра (`heartbeat_*.log`)
- Мониторьте логи {{ apacheIgniteVariants }} (`igniteClient_*.log`)

**Проверьте кластерные компоненты:**

- Проверьте топологию кластера {{ apacheIgniteVariants }} через инструменты управления
- Используйте эндпоинт `/_cluster/health` для проверки состояния {{ openSearchVariants }}
- Контролируйте состояние топиков и партиций {{ apacheKafkaVariants }}

**Анализируйте логи:**

- Ищите критические ошибки и предупреждения
- Проверьте сообщения о ребалансировке кластера
- Мониторьте производительность операций

## Устранение неполадок {: #cluster_recovery_troubleshooting .pageBreakBefore }

### Частые проблемы при восстановлении {: #cluster_recovery_common_issues }

- **Узел не подключается к кластеру {{ apacheIgniteVariants }}**: проверьте сетевое подключение между узлами, конфигурацию {{ apacheIgniteVariants }}, файрвол и сетевые настройки.
- **Ошибки синхронизации данных**: проверьте доступность NFS-сервера, права доступа к файлам, состояние дискового пространства, конфигурацию монтирования в `/etc/fstab`.
- **Проблемы с балансировщиком нагрузки**: проверьте конфигурацию балансировщика, доступность эндпоинта `api/health`, настройки проверки состояния узлов.
- **Проблемы с NFS-монтированием**: проверьте доступность NFS-сервера, настройки в `/etc/fstab`, права доступа к общим директориям.

### Логи для диагностики {: #cluster_recovery_logs }

**Основные логи {{ productName }}:**

- `heartbeat_*.log` — состояние экземпляра и процесс запуска
- `igniteClient_*.log` — подключение к {{ apacheIgniteVariants }} и ребалансировка кластера
- `application.log` — общие ошибки приложения
- `Workers.config` — конфигурация рабочих процессов узла

**Логи инфраструктуры:**

- Логи {{ apacheIgniteVariants }} — состояние кластера данных
- Логи {{ openSearchVariants }} — состояние поискового кластера
- Логи {{ apacheKafkaVariants }} — состояние очередей сообщений

### Инструменты диагностики {: #cluster_recovery_diagnostic_tools }

**Проверка состояния узлов:**

```sh
curl -X GET "http://<node-ip>/api/health"
```

**Проверка {{ apacheIgniteVariants }}:**

```sh
bash /usr/share/ignite/bin/control.sh --baseline
```

**Проверка {{ openSearchVariants }}:**

```sh
curl -X GET "localhost:9200/_cluster/health?pretty"
```

**Проверка {{ apacheKafkaVariants }}:**

```sh
kafka-topics.sh --bootstrap-server localhost:9092 --list
```

## Стратегические принципы восстановления {: #cluster_recovery_strategic_principles .pageBreakBefore }

### Подготовка к авариям {: #cluster_recovery_preparation_philosophy }

Системы, которые регулярно тестируют восстановление, восстанавливаются быстрее и надёжнее:
- Команда знает процедуры и не тратит время на их изучение
- Выявлены и устранены проблемы в процессе тестирования
- Создана культура готовности к нештатным ситуациям

### Стратегия резервного копирования {: #cluster_recovery_backup_strategy }

**Многоуровневый подход к резервному копированию:**

Современные системы требуют многоуровневого подхода к резервному копированию, где каждый уровень решает свои задачи:

- **Полные резервные копии** (еженедельно для продуктивных систем, ежемесячно для тестовых) — обеспечивают точку восстановления для критических ситуаций
- **Инкрементальные копии** (ежедневно для всех систем) — минимизируют потерю данных при ежедневных сбоях
- **Платформенные копии** (каждые 4-12 часов в зависимости от критичности) — обеспечивают быстрое восстановление конфигурации и данных приложения
- **Различные периоды хранения** (от 1 дня до 1 месяца) — балансируют между требованиями к хранению и стоимостью

**Детализированная стратегия резервного копирования по контурам:**

**Продуктивный контур:**
- **Полные копии**: еженедельно (воскресенье), хранение 1 месяц
- **Инкрементальные копии**: ежедневно (00:00-03:00), хранение 2 недели
- **Платформенные копии**: каждые 4 часа, хранение 1 день
- **Автоматизация**: VEEAM для полных и инкрементальных копий, платформа {{ productName }} для платформенных копий

**Препродакшен контур:**
- **Полные копии**: ежемесячно, хранение 1 месяц
- **Инкрементальные копии**: ежедневно (00:00-03:00), хранение 2 недели
- **Платформенные копии**: каждые 12 часов, хранение 3 дня

**Тестовый контур:**
- **Полные копии**: ежемесячно, хранение 1 месяц
- **Инкрементальные копии**: ежедневно (00:00-03:00), хранение 2 недели
- **Платформенные копии**: каждые 8 часов, хранение 2 дня


### Тестирование восстановления {: #cluster_recovery_testing }

Регулярное тестирование помогает:
- Выявить проблемы в процедурах восстановления
- Обучить команду работе в нештатных ситуациях
- Проверить актуальность резервных копий
- Оценить время восстановления (RTO) и потери данных (RPO)

**Сценарии тестирования:**

- **Восстановление одного узла** — наиболее частый сценарий, требует регулярного тестирования
- **Полное восстановление кластера** — критический сценарий, тестируется реже, но более тщательно
- **Восстановление по приоритету** — проверяет готовность к частичным сбоям
- **Восстановление в различных условиях** — тестирует устойчивость процедур к различным сценариям

### Мониторинг {: #cluster_recovery_monitoring_culture }

Эффективный мониторинг помогает:
- Предотвратить сбои до их возникновения
- Быстро диагностировать проблемы при их возникновении
- Оценить эффективность восстановительных процедур

**Компоненты мониторинга:**

- **Автоматические уведомления** о критических событиях
- **Регулярные проверки** состояния компонентов
- **Журнал инцидентов**
- **Мониторинг резервных копий**

### Специфика восстановления различных архитектур {: #cluster_recovery_architectures }

**Кластер из толстых узлов:**

- Все узлы имеют полную функциональность
- Восстановление может выполняться в любом порядке
- Убедитесь в правильности конфигурации рабочих процессов

**Кластер с разноролевыми узлами:**

- Восстанавливайте серверные узлы перед клиентскими
- Проверьте конфигурацию тонких клиентов
- Убедитесь в доступности серверных узлов для клиентских

**Кластер с дополнительными узлами {{ apacheIgniteVariants }}:**

- Восстанавливайте узлы {{ apacheIgniteVariants }} перед узлами приложений
- Проверьте топологию кластера {{ apacheIgniteVariants }}
- Убедитесь в правильности конфигурации портов

### Сценарии восстановления по компонентам {: #cluster_recovery_component_scenarios }

**Восстановление основного узла кластера:**

- Требует полной остановки кластера
- Необходима реконфигурация всех дополнительных узлов
- Восстановление данных из последней резервной копии
- Последовательный запуск узлов с реконфигурацией кластера

**Восстановление дополнительного узла кластера:**

- Может выполняться без остановки основного узла
- Восстановление конфигурации и данных узла
- Подключение к существующему кластеру
- Синхронизация данных с основным узлом

**Восстановление инфраструктурных компонентов:**

- **Серверы логирования**: восстановление {{ openSearchVariants }} и {{ apacheKafkaVariants }}
- **Файловые серверы**: восстановление NFS-сервера и общих директорий
- **База данных**: восстановление данных экземпляра приложения

**Восстановление по критичности:**

- **Критичные компоненты**: основные узлы приложений, база данных
- **Важные компоненты**: серверы логирования, файловое хранилище
- **Вспомогательные компоненты**: мониторинг, дополнительные сервисы

### Специфические сценарии восстановления {: #cluster_recovery_specific_scenarios }

**Полное восстановление основного узла кластера:**

При отказе основного узла кластера (например, S201AS-BPM01) требуется полная остановка кластера и последовательное восстановление всех узлов:

- Остановите все службы на остальных узлах кластера (Nginx → Ignite → API Gateway → Application)
- Восстановите основной узел из резервной копии
- Восстановите данные приложения и конфигурацию
- Запустите основной узел и проверьте его работоспособность
- Последовательно восстановите дополнительные узлы с реконфигурацией кластера
- Выполните проверку целостности данных и функциональности

**Полное восстановление дополнительного узла кластера:**

При отказе дополнительного узла (например, S201AS-BPM02 или S201AS-BPM03) восстановление может выполняться без остановки основного узла:

- Восстановите узел из резервной копии
- Очистите локальную базу данных узла
- Настройте ссылки на общие файлы (Scripts)
- Запустите службы в правильном порядке
- Выполните реконфигурацию кластера для подключения узла
- Проверьте синхронизацию данных с основным узлом

**Частичное восстановление сервера приложений:**

При незначительных сбоях, не требующих полного восстановления:

- Проверьте настройки и доступы согласно документации
- Перезапустите службы в правильном порядке при необходимости
- Проверьте подключение к кластерным компонентам
- Убедитесь в работоспособности приложения

**Восстановление сервера логирования:**

- **Полное восстановление**: восстановите сервер из резервной копии, службы {{ openSearchVariants }} запустятся автоматически
- **Частичное восстановление**: проверьте настройки и перезапустите службу {{ openSearchVariants }} при необходимости

**Восстановление файлового хранилища:**

- Восстановите файловый сервер из резервной копии
- Перезапустите службы на всех узлах приложений поочерёдно
- Проверьте доступность файлов через веб-интерфейс
- Убедитесь в работоспособности управления файлами

**Восстановление базы данных экземпляра приложения:**

- Остановите все узлы кластера
- Восстановите данные из резервной копии платформы (формат .cdbbz)
- Восстановите файлы скриптов и потоков в общее хранилище
- Настройте ссылки на общие файлы
- Запустите узлы в правильном порядке с реконфигурацией кластера
- Проверьте целостность данных и функциональность

### Восстановление по типам контуров {: #cluster_recovery_environment_scenarios }

**Продуктивный контур:**

- Требует наиболее тщательного планирования и тестирования
- Использует полные резервные копии еженедельно
- Инкрементальные копии ежедневно
- Платформенные копии каждые 4 часа
- Максимальный период хранения резервных копий

**Препродакшен контур:**

- Менее критичен, но требует регулярного тестирования
- Полные копии ежемесячно
- Инкрементальные копии ежедневно
- Платформенные копии каждые 12 часов
- Средний период хранения резервных копий

**Тестовый контур:**

- Наименее критичен, но важен для разработки
- Полные копии ежемесячно
- Инкрементальные копии ежедневно
- Платформенные копии каждые 8 часов
- Минимальный период хранения резервных копий

### Практики, которых следует избегать {: #cluster_recovery_anti_patterns }

- **Не игнорируйте резервное копирование**: нерегулярное или неправильное резервное копирование данных может осложнить восстановление системы после сбоя.
- **Не пропускайте тестирование восстановления**: невыполнение регулярного тестирования процедур восстановления может привести к неготовности к авариям.
- **Не восстанавливайте несколько узлов одновременно**: одновременное восстановление нескольких узлов может привести к конфликтам данных.
- **Не игнорируйте проверки целостности**: всегда проверяйте целостность данных после восстановления.
- **Не нарушайте конфигурацию рабочих процессов**: только один узел должен выполнять критические фоновые задачи.

- **Не нарушайте порядок остановки и запуска служб**: неправильный порядок может привести к повреждению данных или неработоспособности кластера.
- **Не игнорируйте реконфигурацию кластера**: после восстановления основного узла необходимо реконфигурировать все дополнительные узлы.
- **Не восстанавливайте компоненты в произвольном порядке**: соблюдайте приоритеты восстановления (критичные → важные → вспомогательные).

<div class="relatedTopics" markdown="block">

--8<-- "related_topics_heading.md"

- [Установка, запуск, инициализация и остановка ПО][deploy_guide_linux]
- [Обновление кластера {{ productName }}][cluster_upgrade]
- [Системные требования {{ productName }}][system_requirements]
- [Резервное копирование. Настройка и запуск, просмотр журнала сеансов][backup_configure]

</div>

{% include-markdown ".snippets/hyperlinks_mkdocs_to_kb_map.md" %}

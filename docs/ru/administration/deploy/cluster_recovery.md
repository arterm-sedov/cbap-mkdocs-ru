---
kbId: 5135
title: Восстановление кластера {{ productName }}
tags:
    - Apache Ignite
    - Grafana
    - Kafka
    - Loki
    - OpenSearch
    - Prometheus
    - администрирование
    - архитектура
    - балансировка нагрузки
    - восстановление
    - кластер
    - кластеризация
    - мониторинг
    - отказоустойчивость
    - развёртывание
    - резервное копирование
    - управление кластером
hide: tags
---

# Восстановление кластера {{ productName }} {: #cluster_recovery }

{% include-markdown ".snippets/experimental_feature.md" %}

## Введение {: #cluster_recovery_introduction }

**{{ productName }}** поддерживает кластерные конфигурации, которые обеспечивают высокую доступность, отказоустойчивость и бесперебойное функционирование.

Здесь представлены рекомендации и процедуры по восстановлению работы кластера **{{ productName }}** после различных типов аварий.

## Архитектура восстановления {: #cluster_recovery_architecture .pageBreakBefore }

**Кластерная архитектура {{ productName }}** обеспечивает многоуровневую систему восстановления:

- **Автоматически перераспределяем трафик** при отказе узлов
- **Реплицируем данные** в кластере {{ apacheIgniteVariants }}
- **Синхронизируем индексы** в кластере {{ openSearchVariants }}
- **Создаём резервные копии** в распределённой файловой системе

_![Пример порядка восстановления кластера](img/cluster_upgrade_recover_deploy_recovery.png)_

### Кластер приложений Platform {: #cluster_recovery_platform_cluster }

**{{ productName }}** развёртывается в виде кластера из пяти узлов приложений:

- **PLATFORM-node1** (XX.XX.101.7) — основной узел интеграции
- **PLATFORM-node2** (XX.XX.101.8) — узел мониторинга и поиска
- **PLATFORM-node3** (XX.XX.101.9) — узел поиска и обработки
- **PLATFORM-node4** (XX.XX.101.18) — узел обработки
- **PLATFORM-node5** (XX.XX.101.19) — узел обработки

_![Пример конфигурации кластера](img/cluster_upgrade_recover_deploy_configuration.png)_

### Компоненты узлов Platform {: #cluster_recovery_platform_components }

Каждый узел содержит следующие компоненты:

- **{{ nginxVariants }}** — обратный прокси-сервер, который обрабатывает HTTP-запросы
- **Adapters** — компоненты, которые интегрируются с внешними системами
- **Async** — асинхронные обработчики, которые выполняют фоновые задачи
- **{{ apacheKafkaVariants }}** — очереди сообщений, которые обеспечивают межсервисное взаимодействие
- **Core Ignite** — распределённая сеть данных, которая кэширует и хранит данные

### Распределение нагрузки {: #cluster_recovery_load_balancing }

**Балансировщик нагрузки** (XX.XX.106.43) распределяет трафик следующим образом:

- **Пользовательский трафик**: PLATFORM-node1 (40%), PLATFORM-node2 (30%), PLATFORM-node3 (20%)
- **Трафик CRM-системы**: распределяется между PLATFORM-node1, PLATFORM-node2, PLATFORM-node3
- **Остальные узлы**: PLATFORM-node4 и PLATFORM-node5 обрабатывают дополнительную нагрузку

**Контроль исправности узлов:**

- Все узлы предоставляют эндпоинт `api/health`
- Статус 200 означает, что узел **{{ productName }}** работает исправно
- Балансировщик автоматически исключает нездоровые узлы из обработки трафика

### Инфраструктура данных {: #cluster_recovery_data_infrastructure }

**Распределённая файловая система:**

- **DFS** (`//dfs-01/picompany.ru/sdapps$`) — общее хранилище для всех узлов
- Все пять узлов подключаются к DFS в двунаправленном режиме

**Кластер поиска:**

- **{{ openSearchVariants }} Cluster** (XX.XX.101.137-139) — кластер поиска для узлов 2-5
- Обеспечивает индексирование и поиск по данным платформы

**Распределённое хранилище данных:**

- **{{ apacheIgniteVariants }} Cluster** — распределённое хранилище, которое реплицирует данные между узлами
- Обеспечивает высокую доступность и производительность данных

## Сценарии восстановления {: #cluster_recovery_scenarios .pageBreakBefore }

### Отказ одного узла приложений {: #cluster_recovery_single_node_failure }

**Автоматическое восстановление:**
- Балансировщик нагрузки автоматически исключает нездоровый узел
- Трафик перераспределяется на оставшиеся узлы
- Кластер {{ apacheIgniteVariants }} продолжает работать с оставшимися узлами

**Ручное восстановление:**
- Диагностируем причину отказа узла
- Восстанавливаем узел или заменяем на новый
- Проверяем подключение к кластерам
- Возвращаем узел в пул балансировщика

**Контрольные точки:**

- Мониторим эндпоинт `api/health` на восстанавливаемом узле
- Проверяем подключение к кластеру Apache Ignite
- Анализируем логи на предмет ошибок
- Проверяем синхронизацию данных с другими узлами

### Отказ кластера {{ apacheIgniteVariants }} {: #cluster_recovery_ignite_failure }

**Немедленные действия:**
- Проверяем состояние всех узлов кластера
- Идентифицируем отказавшие узлы
- Проверяем целостность данных

**Восстанавливаем кластер:**
- Перезапускаем отказавшие узлы
- Проверяем синхронизацию данных
- Восстанавливаем из резервных копий при необходимости

**Проверяем работоспособность:**
- Тестируем операции чтения/записи
- Проверяем производительность кластера
- Мониторим стабильность работы

**Контрольные точки:**

- Проверяем топологию кластера через инструменты управления
- Мониторим состояние репликации данных
- Контролируем производительность операций чтения/записи

### Отказ кластера {{ openSearchVariants }} {: #cluster_recovery_opensearch_failure }

**Диагностируем:**
- Проверяем состояние узлов кластера
- Анализируем логи для выявления причины
- Проверяем доступность индексов

**Восстанавливаем:**
- Перезапускаем отказавшие узлы
- Проверяем целостность индексов
- Восстанавливаем индексы из резервных копий

**Проверяем поиск:**
- Тестируем поисковые запросы
- Проверяем производительность индексирования
- Мониторим работу кластера

**Контрольные точки:**

- Используем эндпоинт `/_cluster/health` для проверки состояния
- Проверяем состояние индексов и их репликации
- Мониторим производительность поисковых запросов

### Отказ кластера {{ apacheKafkaVariants }} {: #cluster_recovery_kafka_failure }

**Диагностируем:**
- Проверяем состояние узлов кластера
- Анализируем логи для выявления причины
- Проверяем доступность топиков

**Восстанавливаем:**
- Перезапускаем отказавшие узлы
- Проверяем целостность топиков и партиций
- Восстанавливаем данные из резервных копий

**Проверяем работу:**
- Тестируем отправку и получение сообщений
- Проверяем производительность обработки
- Мониторим работу кластера

**Контрольные точки:**

- Контролируем состояние топиков и партиций
- Мониторим задержку обработки сообщений
- Проверяем размер очередей сообщений

### Полный отказ кластера {: #cluster_recovery_complete_failure }

**Активируем резервное копирование:**
- Восстанавливаем данные из последней резервной копии
- Восстанавливаем конфигурацию системы
- Восстанавливаем файлы из DFS

**Поэтапно восстанавливаем:**
- Восстанавливаем кластер {{ apacheIgniteVariants }}
- Восстанавливаем кластер {{ openSearchVariants }}
- Восстанавливаем кластер {{ apacheKafkaVariants }}
- Восстанавливаем узлы приложений
- Настраиваем балансировщик нагрузки

**Проверяем систему:**
- Полностью тестируем функциональность
- Проверяем производительность
- Мониторим стабильность работы

**Контрольные точки:**

- Проверяем все эндпоинты `api/health`
- Тестируем все основные функции системы
- Мониторим производительность всех компонентов

## Процедуры восстановления {: #cluster_recovery_procedures .pageBreakBefore }

### Диагностика проблем {: #cluster_recovery_diagnosis }

**Проверяем состояние узлов:**
- Используем эндпоинт `api/health` для проверки состояния узлов приложений
- Анализируем логи состояния экземпляра (`heartbeat_*.log`)
- Мониторим логи Apache Ignite (`igniteClient_*.log`)

**Проверяем кластерные компоненты:**
- Проверяем топологию кластера Apache Ignite через инструменты управления
- Используем эндпоинт `/_cluster/health` для проверки состояния OpenSearch
- Контролируем состояние топиков и партиций Kafka

**Анализируем логи:**
- Ищем критические ошибки и предупреждения
- Проверяем сообщения о ребалансировке кластера
- Мониторим производительность операций

### Восстановление узла приложения {: #cluster_recovery_node_restoration }

**Останавливаем проблемный узел:**

```sh
systemctl stop comindware<instanceName>
```

**Диагностируем проблему:**
- Анализируем логи на предмет ошибок
- Проверяем состояние файловой системы
- Проверяем доступность сетевых ресурсов

**Восстанавливаем узел:**
- Исправляем выявленные проблемы
- При необходимости восстанавливаем из резервной копии
- Проверяем конфигурацию узла

**Запускаем узел:**

```sh
systemctl start comindware<instanceName>
```

**Проверяем восстановление:**
- Мониторим эндпоинт `api/health`
- Проверяем подключение к кластеру Apache Ignite
- Анализируем логи на предмет ошибок
- Проверяем синхронизацию данных с другими узлами

### Восстановление кластера Apache Ignite {: #cluster_recovery_ignite_restoration }

**Проверяем состояние кластера:**

```sh
bash /usr/share/ignite/bin/control.sh --baseline
```

**Восстанавливаем отказавшие узлы:**
- Перезапускаем сервисы Apache Ignite
- Проверяем подключение к кластеру
- Мониторим процесс ребалансировки

**Проверяем восстановление:**
- Дожидаемся сообщения: `INFO Skipping rebalancing (nothing scheduled)`
- Проверяем топологию кластера
- Тестируем операции чтения/записи

### Восстановление кластера OpenSearch {: #cluster_recovery_opensearch_restoration }

**Проверяем состояние кластера:**

```sh
curl -X GET "localhost:9200/_cluster/health?pretty"
```

**Восстанавливаем отказавшие узлы:**
- Перезапускаем сервисы OpenSearch
- Проверяем подключение к кластеру
- Мониторим процесс восстановления индексов

**Проверяем восстановление:**
- Проверяем состояние индексов
- Тестируем поисковые запросы
- Мониторим производительность индексирования

## Мониторинг и контроль восстановления {: #cluster_recovery_monitoring .pageBreakBefore }

### Ключевые эндпоинты для мониторинга {: #cluster_recovery_monitoring_endpoints }

**Эндпоинт здоровья узлов:**

- **`api/health`**: проверяем состояние узлов приложений
  - Статус 200 OK — узел работает корректно
  - Время отклика не должно превышать 5 секунд
  - Регулярно проверяем каждые 30 секунд

**Кластер Apache Ignite:**

- Проверяем топологию кластера через инструменты управления
- Мониторим состояние репликации данных
- Контролируем производительность операций чтения/записи

**Кластер OpenSearch:**

- Используем эндпоинт `/_cluster/health` для проверки состояния
- Проверяем состояние индексов и их репликации
- Мониторим производительность поисковых запросов

**Кластер Kafka:**

- Контролируем состояние топиков и партиций
- Мониторим задержку обработки сообщений
- Проверяем размер очередей сообщений

### Системы оповещения {: #cluster_recovery_alerting_systems }

**Инструменты мониторинга:**

- **Prometheus + Grafana**: мониторим метрики и визуализируем данные
- **Loki**: агрегируем и анализируем логи
- **Zabbix**: комплексно мониторим инфраструктуру

**Критические события для оповещения:**

- Отказ узла приложения (статус `api/health` != 200)
- Превышение пороговых значений производительности
- Ошибки в кластерах Apache Ignite или OpenSearch
- Проблемы с балансировкой нагрузки
- Критические ошибки в системных логах

### Факторы для рассмотрения при мониторинге {: #cluster_recovery_monitoring_factors }

**Производительность:**

- Измеряем время отклика каждого узла
- Контролируем использование ресурсов (CPU, память, дисковое пространство)
- Мониторим сетевую активность между узлами кластера

**Надёжность:**

- Анализируем критические ошибки и предупреждения в логах
- Проверяем равномерность распределения запросов
- Контролируем стабильность работы кластерных компонентов

## Рекомендации по эксплуатации кластера {: #cluster_recovery_operations .pageBreakBefore }

### Планируем восстановление {: #cluster_recovery_planning }

1. **Создаём план восстановления**:

   - Документируем процедуры восстановления для каждого сценария
   - Определяем роли и ответственность команды
   - Подготавливаем контактную информацию

2. **Тестируем процедуры восстановления**:

   - Регулярно тестируем сценарии восстановления
   - Проверяем актуальность резервных копий
   - Тренируем команду на процедурах восстановления

3. **Мониторим состояние системы**:

   - Настраиваем автоматические уведомления о проблемах
   - Регулярно проверяем состояние всех компонентов
   - Ведём журнал инцидентов и их разрешения

### Обеспечиваем безопасность кластера {: #cluster_recovery_security }

1. **Настраиваем сетевую безопасность**:

   - Используем WAF для защиты от атак
   - Настраиваем межсетевые экраны между зонами
   - Шифруем трафик между узлами кластера

2. **Контролируем доступ**:

   - Настраиваем аутентификацию для всех компонентов
   - Используем принцип минимальных привилегий
   - Регулярно обновляем пароли и сертификаты

3. **Мониторим безопасность**:

   - Отслеживаем подозрительную активность
   - Анализируем логи на предмет атак
   - Настраиваем автоматические уведомления о нарушениях

## Практики, которых следует избегать {: #cluster_recovery_anti_patterns .pageBreakBefore }

Для обеспечения оптимальной производительности, минимизации рисков простоев и повышения надёжности, доступности и отказоустойчивости **{{ productName }}** избегаем следующих подходов:

- **Игнорируем мониторинг и оповещения**: отсутствие постоянного мониторинга состояния системы может привести к пропуску критических инцидентов и увеличению времени простоя
- **Недостаточно реплицируем данные**: хранение данных только на одном узле повышает риск их утраты в случае сбоя
- **Не создаём резервные копии**: нерегулярное или неправильное резервное копирование данных может осложнить восстановление системы после сбоя
- **Пренебрегаем тестированием отказоустойчивости**: отсутствие регулярного тестирования сценариев отказа системы приводит к неготовности к устранению неполадок в критических ситуациях
- **Неоптимально настраиваем конфигурацию**: использование дефолтных или неподходящих настроек ПО может привести к увеличению задержек и снижению производительности
- **Развёртываем все узлы в одной зоне**: отсутствие географического распределения узлов повышает уязвимость системы к локальным сбоям или катастрофам
- **Недооцениваем нагрузку**: неправильная оценка будущих нагрузок может привести к сбоям системы при увеличении объёмов данных, количества транзакций или пользователей
- **Недостаточно тестируем восстановление**: отсутствие регулярного тестирования процедур восстановления может привести к неготовности к реальным авариям

<div class="relatedTopics" markdown="block">

--8<-- "related_topics_heading.md"

- [Установка, запуск, инициализация и остановка ПО][deploy_guide_linux]
- [Обновление кластера {{ productName }}][cluster_upgrade]
- [Системные требования {{ productName }}][system_requirements]
- [Резервное копирование. Настройка и запуск, просмотр журнала сеансов][backup_configure]

</div>

{% include-markdown ".snippets/hyperlinks_mkdocs_to_kb_map.md" %}
